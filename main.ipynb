{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning:\n",
      "\n",
      "`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\n",
      "/tmp/ipykernel_9735/3130040250.py:99: DeprecationWarning:\n",
      "\n",
      "`recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import PointStruct, VectorParams, Distance\n",
    "import re\n",
    "\n",
    "# Initialize the model for vectorizing text\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = []\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text.append(page.get_text(\"text\"))\n",
    "        # Check for images in the page\n",
    "        image_list = page.get_images(full=True)\n",
    "        for img_index, img in enumerate(image_list):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image = Image.open(BytesIO(image_bytes))\n",
    "            # Use pytesseract to do OCR on the image\n",
    "            text.append(pytesseract.image_to_string(image))\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# Function to split text into sentences\n",
    "def split_text_into_sentences(text):\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    return sentences\n",
    "\n",
    "# Function to vectorize the text\n",
    "def vectorize_text(sentences):\n",
    "    return model.encode(sentences)\n",
    "\n",
    "# Function to store vectors in Qdrant\n",
    "def store_vectors_in_qdrant(vectors, texts, qdrant_client, collection_name):\n",
    "    points = [\n",
    "        PointStruct(\n",
    "            id=i,\n",
    "            vector=vector.tolist(),\n",
    "            payload={\"text\": text}\n",
    "        )\n",
    "        for i, (vector, text) in enumerate(zip(vectors, texts))\n",
    "    ]\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        points=points\n",
    "    )\n",
    "\n",
    "# Function to fetch vector from a given word input\n",
    "def fetch_vector_from_word(word):\n",
    "    vector = model.encode(word)\n",
    "    return vector\n",
    "\n",
    "# Function to fetch the closest word(s) from vector input\n",
    "def fetch_word_from_vector(vector, qdrant_client, collection_name, top_k=1):\n",
    "    search_result = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=vector.tolist(),\n",
    "        limit=top_k,\n",
    "        with_payload=True\n",
    "    )\n",
    "    return [hit.payload[\"text\"] for hit in search_result]\n",
    "\n",
    "# Function to fetch vector directly from Qdrant using word input\n",
    "def fetch_vector_from_qdrant_by_word(word, qdrant_client, collection_name, top_k=1):\n",
    "    vector = fetch_vector_from_word(word)\n",
    "    closest_words = fetch_word_from_vector(vector, qdrant_client, collection_name, top_k=top_k)\n",
    "    return closest_words\n",
    "\n",
    "# Main function to process the PDF and store vectors\n",
    "def process_pdf(pdf_path, qdrant_client, collection_name):\n",
    "    # Extract text from PDF\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    sentences = split_text_into_sentences(text)\n",
    "    \n",
    "    # Vectorize the text\n",
    "    vectors = vectorize_text(sentences)\n",
    "    \n",
    "    # Store vectors in Qdrant\n",
    "    store_vectors_in_qdrant(vectors, sentences, qdrant_client, collection_name)\n",
    "\n",
    "# Example usage\n",
    "pdf_path = 'example.pdf'\n",
    "collection_name = 'pdf_texts'\n",
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"https://22d49f19-31bc-4841-9696-77d665b462be.us-east4-0.gcp.cloud.qdrant.io:6333\", \n",
    "    api_key=\"YYT-1bIz6NjE0ZGY0f0zMMQH3uJe-rrB8707Q7OLLhBcQC8ZBH7hNg\",\n",
    ")\n",
    "\n",
    "# Create a collection in Qdrant (if it doesn't exist)\n",
    "qdrant_client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.COSINE)  # Adjust the size to match your vector dimensions\n",
    ")\n",
    "\n",
    "# Process the PDF and store vectors\n",
    "process_pdf(pdf_path, qdrant_client, collection_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest text(s) in Qdrant to 'Venture':\n",
      "Additionally, participants \n",
      "have the opportunity to forge connections within the vibrant Venture \n",
      "Development Centre community, enabling them to explore avenues \n",
      "conducive to crafting a robust, customer-centric startup from inception.\n",
      "\n",
      "• Certificate from Venture Development Center in collaboration with \n",
      "entrepreneurial bodies : Northeastern University-Center for Emerging \n",
      "Markets  (Boston, USA)  , Centrep-Malaysia , Tie Vizag, i-TBI, G-TEC \n",
      "• Access to advanced Maker Space, MURTI Lab and other facilities at \n",
      "GITAM.\n",
      "Designed to cater to a diverse array of aspiring studentprenuers, this initiative \n",
      "welcomes individuals from various sectors who exhibit the drive and creativity \n",
      "to innovate, execute, and establish impactful ventures.\n",
      "• Visit to AMTZ to gain exposure the bio -medical start-up ecosystem\n",
      "• Extensive network of mentors for entrepreneurial ventures.\n",
      "(Go To \n",
      "Market Strategy, Marketing Plan & Customer Acquisition) \n",
      "Day 7: Financial Management & Funding: Understand financial \n",
      "basics for your venture, explore funding options, and build a financial \n",
      "model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word = \"Venture\"\n",
    "closest_texts = fetch_vector_from_qdrant_by_word(word, qdrant_client, collection_name, top_k=5)\n",
    "print(f\"Closest text(s) in Qdrant to '{word}':\")\n",
    "for i in closest_texts:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Work in Progress\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dummy vector with 384 dimensions\n",
    "dummy_vector = [0] * 384\n",
    "\n",
    "# Fetch all vectors from Qdrant\n",
    "search_result = qdrant_client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=dummy_vector,\n",
    "    limit=1000000,  # Set a high limit\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "# Extract vectors and texts from search result\n",
    "vectors = []\n",
    "texts = []\n",
    "for hit in search_result:\n",
    "    if hit.vector is not None:\n",
    "        vectors.append(hit.vector)\n",
    "        texts.append(hit.payload[\"text\"])\n",
    "\n",
    "print(texts)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
